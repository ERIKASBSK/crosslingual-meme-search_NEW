import os
import numpy as np
import pandas as pd
import streamlit as st
from sentence_transformers import SentenceTransformer

MODEL = "intfloat/multilingual-e5-base"
CSV_PATH = "data/jpmemes.csv"
# epsã¯ã»ã¼ä¿é™ºã®ã‚ˆã†ãªã‚‚ã®ã€ï¼ã ã‘ã˜ã‚ƒåˆ†ã‹ã‚‰ãªã„ã®ã§ã€norm = 0ã™ã‚‹ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒ è‡ªä½“çˆ†ç™ºã—ã¡ã‚ƒã†
# è©³ã—ã„èª¬æ˜ã¯google drive ã®ãƒãƒ¼ãƒˆã§ãƒ¡ãƒ¢ã—ã¾ã—ãŸ
EPS = 1e-12

# ã‚ˆãä½¿ã†ãƒãƒƒãƒˆã‚¹ãƒ©ãƒ³ã‚°ãƒªã‚¹ãƒˆ - çŸ­ã„ã‘ã©OK
ALLOW_SHORT = {
    "lol", "lmao", "rofl", "omg", "wtf", "idk", "ikr", "ngl", "fr", "tbh",
    "sus", "cap", "bet", "bruh", "gg", "w", "l", "nsfw", "jk", "pls", "thx", "wtf"
}

@st.cache_resource
def load_model():
    # ä¸€å›ã ã‘ãƒ­ãƒ¼ãƒ‰ã™ã‚Œã°streamlitãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ãã‚Œã‚‹
    return SentenceTransformer(MODEL)

def norm_rows(x):
    """æ­£è¦åŒ–"""
    #we could try float64 when i buy a new pc tho....ram is not good enough
    x = np.asarray(x, dtype=np.float32)
    norms = np.linalg.norm(x, axis=1, keepdims=True)
    # ãŸã¾ã«ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚ã‚‹ã‹ã‚‰ä¸€å¿œepsã«ãŠãã‹ãˆã‚‹ãƒ¼
    norms = np.where(norms > 0, norms, EPS)
    return x / norms

# stackoverflowã®èª¬æ˜å‚è€ƒã—ãªãŒã‚‰æ›¸ã„ãŸã§ã™ã‘ã©ã€‚ã€‚å°†æ¥çš„ã«å‚è€ƒã«ãªã‚Œã‚‹
# https://stackoverflow.com/questions/78879594/st-cache-data-is-deprecated-in-streamlit

@st.cache_data
def load_data_and_emb(csv_mtime):
    """CSVèª­ã‚“ã§embeddingè¨ˆç®—ã€‚mtimeãŒå¤‰ã‚ã£ãŸã‚‰å†è¨ˆç®—"""
    df = pd.read_csv(CSV_PATH)

    # NaNå‡¦ç†ã€jp_textã®å­˜åœ¨ã¯å¿…è¦
    if "jp_text" not in df.columns:
        st.error("No 'jp_text' column in CSV...")
        st.stop()

    df["jp_text"] = df["jp_text"].fillna("").astype(str)

    # sourceâ€•Excelã®column+ img
    if "source" in df.columns:
        df["source"] = df["source"].fillna("").astype(str)
    else:
        df["source"] = ""

    if "img" in df.columns:
        df["img"] = df["img"].fillna("").astype(str)
    else:
        df["img"] = ""

    # cï½“ï½–ã®å‡¦ç†
    passages = []
    for idx, row in df.iterrows():
        txt = row["jp_text"].strip()
        passages.append(f"passage: {txt}")

    model = load_model()
    emb = model.encode(passages, batch_size=64, show_progress_bar=False)
    emb = norm_rows(emb)

    return {
        "emb": emb,
        "jp_text": df["jp_text"].to_numpy(),
        "source": df["source"].to_numpy(),
        "img": df["img"].to_numpy(),
    }

def embed_query(model, q):
    """E5å®‰å®šç‰ˆfor the accuracy"""
    q = q.strip()
    v = model.encode([f"query: {q}"], show_progress_bar=False)
    v = norm_rows(v)
    return v[0]

# cosine similarity, lambda
def search_topk(query_vec, emb, k):
    """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ"""
    scores = emb.dot(query_vec)

    # kãŒå¤‰ãªå€¤æ¥ãªã„ã‚ˆã†ã«
    k = min(k, len(scores))
    if k <= 0:
        return np.array([]), np.array([])

    # ä¸Šä½kå€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    top_idx = np.argsort(scores)[::-1][:k]

    return top_idx, scores[top_idx]

def mmr_select(query_vec, emb, candidate_idx, k, lam=0.75):
    """
    Î»ï¼ˆãƒ©ãƒ ãƒ€ï¼‰ã«ã¤ã„ã¦
    Î»ãŒå¤§ãã„ -> ç²¾åº¦é‡è¦–
    Î»ãŒå°ã•ã„ -> å¤šæ§˜æ€§é‡è¦– 
    """
    candidate_idx = list(map(int, candidate_idx))
    if not candidate_idx or k <= 0:
        return []

    # relevance: cosine(query, doc) â‰ˆ dot(query_vec, doc_vec)
    rel = emb[candidate_idx].dot(query_vec)

    selected = []

    while len(selected) < k and candidate_idx:
        if not selected:
            best_pos = int(np.argmax(rel))
        else:
            cand_emb = emb[candidate_idx]     
            sel_emb = emb[selected]           
  
            sim_to_selected = cand_emb.dot(sel_emb.T)   
            max_sim = sim_to_selected.max(axis=1)       

            mmr_scores = lam * rel - (1 - lam) * max_sim
            best_pos = int(np.argmax(mmr_scores))

        best_idx = candidate_idx[best_pos]
        selected.append(best_idx)

        
        candidate_idx.pop(best_pos)
        rel = np.delete(rel, best_pos)

    return selected

def junk(q: str) -> bool:
    """çŸ­ã™ãã‚‹ã¨ã‹è¨˜å·ã°ã£ã‹ã®ã‚„ã¤å¼¾ã"""
    q = q.strip()
    if len(q) < 3:
        return True

    # è‹±æ•°å­—ãŒå°‘ãªã™ããŸã‚‰NG
    alnum_count = sum(1 for c in q if c.isalnum())
    threshold = max(2, int(len(q) * 0.3))

    return alnum_count < threshold

# ========== UI =================================================================

st.set_page_config(page_title="EN â†’ JP Meme Search", layout="wide")
st.title("EN â†’ JP meme search")

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
if not os.path.exists(CSV_PATH):
    st.error(f"CSV file not found: {CSV_PATH}")
    st.stop()

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
mtime = os.path.getmtime(CSV_PATH)
data = load_data_and_emb(mtime)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("Settings")
    k = st.slider("How many results", 1, 50, 12)
    min_score = st.slider("Match strictness", 0.0, 1.0, 0.55, 0.01)
    strict = st.checkbox("Ignore weird input", value=True)
    use_mmr = st.checkbox("Use MMR(Under testing. Please refrain from use for now.)", value=False)
    mmr_lam = st.slider("MMR lambda", 0.50, 0.95, 0.75, 0.01)


# æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹
q = st.text_input(
    "Type English meme/slang",
    placeholder="cringe / no cap / that's so real / touch grass / delulu ..."
)

if st.button("Search", type="primary"):
    q = q.strip()

    if not q:
        st.warning("Type something pls")
        st.stop()

    # ã‚´ãƒŸå…¥åŠ›ãƒã‚§ãƒƒã‚¯
    q_lower = q.lower()
    if strict:
        if q_lower not in ALLOW_SHORT and junk(q):
            st.warning("Maybe type a bit longer phrase...")
            st.stop()

    # ====== æ¤œç´¢å®Ÿè¡Œã™ã‚‹ã‚ˆ ======
    model = load_model()
    qv = embed_query(model, q)

    # MMRä½¿ã†ãªã‚‰å€™è£œã¯å¤šã‚ã«å–ã£ã¨ãï¼ˆé¸ã¹ã‚‹ä½™åœ°ãŒå¤§äº‹ãªã®ï¼‰
    candidate_pool = max(80, k * 10)
    idx, scores = search_topk(qv, data["emb"], candidate_pool)

    # ã‚¹ã‚³ã‚¢ä½ã™ãã‚‹ã®ã¯å…ˆã«è½ã¨ã™ã‚ˆ
    cand = [(int(i), float(s)) for i, s in zip(idx, scores) if s >= min_score]

    if not cand:
        st.info("No match. Try other words or lower the strictness.")
        st.stop()

    cand_idx = np.array([i for i, _ in cand], dtype=int)

    # MMRåŒã˜ã®ã°ã£ã‹å‡ºã‚‹å•é¡Œã‚’å›é¿ã™ã‚‹ã‚„ã¤
    if use_mmr:
        final_idx = mmr_select(qv, data["emb"], cand_idx, k=k, lam=mmr_lam)
    else:
        # MMRãªã—ãªã‚‰æ™®é€šã«ä¸Šã‹ã‚‰kå€‹ã­
        final_idx = cand_idx[:k].tolist()

    if not final_idx:
        st.info("No match after filtering. Try lower strictness.")
        st.stop()

    st.success(f"Found {len(final_idx)} results")

    # ====== çµæœè¡¨ç¤ºã ã‚ˆã€œ ======
    for rank, i in enumerate(final_idx, 1):
        jp_text = data["jp_text"][i]
        source_tag = data["source"][i]
        img = str(data["img"][i]).strip()

        # è¡¨ç¤ºç”¨ã«ã‚¹ã‚³ã‚¢ã‚’ã‚‚ã†ä¸€å›è¨ˆç®—ï¼ˆæ­£è¦åŒ–ã—ã¦ã‚‹ã‹ã‚‰dotã§OK + image no setting
        score = float(data["emb"][i].dot(qv))

        with st.container():
            st.markdown(f"### Rank {rank} â€” match {score*100:.1f}%")

            if img:
                try:
                    st.markdown("<div style='text-align: center;'>", unsafe_allow_html=True)
                    st.image(img, width=320)
                    st.markdown("</div>", unsafe_allow_html=True)
                except Exception:
                    st.caption("ğŸ–¼ï¸ image link broken / blocked")

            st.write(jp_text)

            if source_tag:
                st.caption(f"ğŸ·ï¸ {source_tag}")

            st.divider()


